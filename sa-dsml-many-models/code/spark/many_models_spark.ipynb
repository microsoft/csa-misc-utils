{"cells":[{"cell_type":"markdown","source":["#Technique in Spark to train multiple models and perform scalable inferencing"],"metadata":{}},{"cell_type":"markdown","source":["## 1.Preperation before class"],"metadata":{}},{"cell_type":"markdown","source":["### Environment preperation\n1. Prepare a Databricks instance with Ls8s_v2\n2. Prepare a Azure ML workspace \n3. Prepare a service principal with secret key registered in keyvault. The service principal should have contributor access to your Azure ML workspace"],"metadata":{}},{"cell_type":"markdown","source":["### Download data from Microsoft Open Dataset"],"metadata":{}},{"cell_type":"markdown","source":["https://azure.microsoft.com/en-us/services/open-datasets/catalog/sample-oj-sales-simulated"],"metadata":{}},{"cell_type":"code","source":["data =spark.read.format(\"csv\").option(\"header\", True).load(\"wasbs://ojsales-simulatedcontainer@azureopendatastorage.blob.core.windows.net/oj_sales_data/Store10*.csv\")"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["#Write to local delta for fast reading\ndata.write.format(\"delta\").saveAsTable(\"OJ_Sales_Data\")"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["%sql optimize OJ_Sales_Data zorder by store, brand"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>metrics</th></tr></thead><tbody><tr><td>null</td><td>List(1, 16, List(456483, 456483, 456483.0, 1, 456483), List(29570, 37082, 36254.5625, 16, 580073), 0, List(minCubeSize(107374182400), List(0, 0), List(16, 580073), 0, List(16, 580073), 1, null), 1, 16, 0, false)</td></tr></tbody></table></div>"]}}],"execution_count":8},{"cell_type":"code","source":["%sql select * from OJ_Sales_Data limit 10"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>WeekStarting</th><th>Store</th><th>Brand</th><th>Quantity</th><th>Advert</th><th>Price</th><th>Revenue</th></tr></thead><tbody><tr><td>1990-06-14</td><td>1094</td><td>minute.maid</td><td>17892</td><td>1</td><td>2.09</td><td>37394.28</td></tr><tr><td>1990-06-21</td><td>1094</td><td>minute.maid</td><td>14053</td><td>1</td><td>2.45</td><td>34429.850000000006</td></tr><tr><td>1990-06-28</td><td>1094</td><td>minute.maid</td><td>17341</td><td>1</td><td>2.47</td><td>42832.270000000004</td></tr><tr><td>1990-07-05</td><td>1094</td><td>minute.maid</td><td>17194</td><td>1</td><td>2.42</td><td>41609.479999999996</td></tr><tr><td>1990-07-12</td><td>1094</td><td>minute.maid</td><td>17945</td><td>1</td><td>2.39</td><td>42888.55</td></tr><tr><td>1990-07-19</td><td>1094</td><td>minute.maid</td><td>17371</td><td>1</td><td>2.3</td><td>39953.299999999996</td></tr><tr><td>1990-07-26</td><td>1094</td><td>minute.maid</td><td>9825</td><td>1</td><td>2.36</td><td>23187.0</td></tr><tr><td>1990-08-02</td><td>1094</td><td>minute.maid</td><td>10849</td><td>1</td><td>2.58</td><td>27990.420000000002</td></tr><tr><td>1990-08-09</td><td>1094</td><td>minute.maid</td><td>12084</td><td>1</td><td>2.0</td><td>24168.0</td></tr><tr><td>1990-08-16</td><td>1094</td><td>minute.maid</td><td>10484</td><td>1</td><td>2.32</td><td>24322.879999999997</td></tr></tbody></table></div>"]}}],"execution_count":9},{"cell_type":"code","source":["%sql select count (distinct store, brand) from OJ_Sales_Data "],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(DISTINCT store, brand)</th></tr></thead><tbody><tr><td>300</td></tr></tbody></table></div>"]}}],"execution_count":10},{"cell_type":"code","source":["%sql select distinct brand from OJ_Sales_Data "],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>brand</th></tr></thead><tbody><tr><td>dominicks</td></tr><tr><td>tropicana</td></tr><tr><td>minute.maid</td></tr></tbody></table></div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["## Pre-training exersize"],"metadata":{}},{"cell_type":"markdown","source":["1. Read about Pandas Function APIs: https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/pandas-function-apis\n2. Answer following questions:\n  - What is the advantage of this technology vs. regular Python UDF?\n  - What is the role of Apache Arrow in this?\n  - What is the use of iterator and yield vs. regular list and return?"],"metadata":{}},{"cell_type":"markdown","source":["Using the OJ sales dataset above, use Pandas Function APIs, pick out for each store and brand the best selling week in the form of week_number-yyyy.\nThe result set look like this:"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nresult_sample= pd.DataFrame({\"store\": [1066, 1067, 1068],'Brand':['dominicks', 'tropicana','tropicana'],\"Best_Selling_Week\": ['23-1992', '24-1991','24-1991']})\ndisplay(result_sample)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>store</th><th>Brand</th><th>Best_Selling_Week</th></tr></thead><tbody><tr><td>1066</td><td>dominicks</td><td>23-1992</td></tr><tr><td>1067</td><td>tropicana</td><td>24-1991</td></tr><tr><td>1068</td><td>tropicana</td><td>24-1991</td></tr></tbody></table></div>"]}}],"execution_count":15},{"cell_type":"code","source":["#Solution\n#The Pandas function\nimport pandas as pd\ndef best_selling_week(inputdf):\n  store =inputdf['Store'][0]\n  brand = inputdf['Brand'][0]\n  best_week_row = inputdf.iloc[inputdf['Quantity'].argmax()]\n  best_week =str(best_week_row['WeekStarting'].isocalendar()[1]) +\"-\"+ str(best_week_row['WeekStarting'].isocalendar()[0])\n  qty = best_week_row['Quantity']\n\n  return pd.DataFrame({\"Store\":[store], \"Brand\":[brand], \"Best_Selling_Week\":best_week, \"Qty\":[qty]})\n  \n\ndf = spark.sql(\"select to_timestamp(WeekStarting) WeekStarting, float(Quantity), Brand,Revenue, Store from OJ_Sales_Data\")\ndf = df.repartition(200) #to increase parallelism\n#Use the pandas function in group by\n\nresult = df.groupby([\"Brand\",\"Store\"]).applyInPandas(best_selling_week, schema=\"Store string, Brand string, Best_Selling_Week string, Qty float\")\ndisplay(result.head(10))"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Store</th><th>Brand</th><th>Best_Selling_Week</th><th>Qty</th></tr></thead><tbody><tr><td>1031</td><td>tropicana</td><td>48-1991</td><td>19916.0</td></tr><tr><td>1021</td><td>minute.maid</td><td>11-1991</td><td>19947.0</td></tr><tr><td>1074</td><td>tropicana</td><td>38-1991</td><td>19932.0</td></tr><tr><td>1077</td><td>minute.maid</td><td>15-1992</td><td>19934.0</td></tr><tr><td>1078</td><td>minute.maid</td><td>44-1991</td><td>19978.0</td></tr><tr><td>1019</td><td>minute.maid</td><td>41-1991</td><td>19685.0</td></tr><tr><td>1090</td><td>tropicana</td><td>44-1990</td><td>19997.0</td></tr><tr><td>1099</td><td>tropicana</td><td>30-1990</td><td>19576.0</td></tr><tr><td>1014</td><td>minute.maid</td><td>32-1991</td><td>19995.0</td></tr><tr><td>1020</td><td>minute.maid</td><td>43-1991</td><td>19996.0</td></tr></tbody></table></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["###Optional reading: we'll forecast models and utilities from the Many Models repo (AML PRS method) to compare. To prepare yourself on the training day, it's useful to get familiar the class and libraries there."],"metadata":{}},{"cell_type":"code","source":["https://github.com/microsoft/solution-accelerator-many-models/blob/master/Custom_Script/scripts/timeseries_utilities.py\nhttps://github.com/microsoft/solution-accelerator-many-models/blob/master/Custom_Script/scripts/train.py\nhttps://github.com/microsoft/solution-accelerator-many-models/blob/master/Custom_Script/scripts/forecast.py"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["##2. Training content"],"metadata":{}},{"cell_type":"markdown","source":["###The Map function"],"metadata":{}},{"cell_type":"markdown","source":["You perform map operations with pandas instances by DataFrame.mapInPandas() in order to transform an iterator of pandas.DataFrame to another iterator of pandas.DataFrame that represents the current PySpark DataFrame and returns the result as a PySpark DataFrame.\n\nThe underlying function takes and outputs an iterator of pandas.DataFrame. It can return the output of arbitrary length in contrast to some pandas UDFs such as Series to Series pandas UDF."],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(' spark.sql.execution.arrow.maxRecordsPerBatch', 100)\n#Default is 10000 which in some cases may defeat the purpose of parallelism"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["def parallel_transform(df_iterator):\n  for df in df_iterator:\n    df['Week'] = df['WeekStarting'].map(lambda x: str(x.isocalendar()[1]) +\"-\"+ str(x.isocalendar()[0]))\n    df.drop(\"WeekStarting\", inplace=True, axis=1)\n    yield df\ndf = spark.sql(\"select to_timestamp(WeekStarting) WeekStarting, float(Quantity), Brand,Revenue, Store from OJ_Sales_Data\")\n\nresult = df.mapInPandas(parallel_transform, schema=\"Store string, Brand string, Week string, Quantity float, Revenue string\")\n\ndisplay(result.head(10))"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Store</th><th>Brand</th><th>Week</th><th>Quantity</th><th>Revenue</th></tr></thead><tbody><tr><td>1094</td><td>minute.maid</td><td>24-1990</td><td>17892.0</td><td>37394.28</td></tr><tr><td>1094</td><td>minute.maid</td><td>25-1990</td><td>14053.0</td><td>34429.850000000006</td></tr><tr><td>1094</td><td>minute.maid</td><td>26-1990</td><td>17341.0</td><td>42832.270000000004</td></tr><tr><td>1094</td><td>minute.maid</td><td>27-1990</td><td>17194.0</td><td>41609.479999999996</td></tr><tr><td>1094</td><td>minute.maid</td><td>28-1990</td><td>17945.0</td><td>42888.55</td></tr><tr><td>1094</td><td>minute.maid</td><td>29-1990</td><td>17371.0</td><td>39953.299999999996</td></tr><tr><td>1094</td><td>minute.maid</td><td>30-1990</td><td>9825.0</td><td>23187.0</td></tr><tr><td>1094</td><td>minute.maid</td><td>31-1990</td><td>10849.0</td><td>27990.420000000002</td></tr><tr><td>1094</td><td>minute.maid</td><td>32-1990</td><td>12084.0</td><td>24168.0</td></tr><tr><td>1094</td><td>minute.maid</td><td>33-1990</td><td>10484.0</td><td>24322.879999999997</td></tr></tbody></table></div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["###Many Model Training"],"metadata":{}},{"cell_type":"code","source":["#prepare values to broadcast\ntenant_id ='72f988bf-86f1-41af-91ab-2d7cd011db47' \nservice_principal_id='af883abf-89dd-4889-bdb3-1ee84f68465e'\nservice_principal_password=dbutils.secrets.get('scope1','app01-pass')\nsubscription_id = '0e9bace8-7a81-4922-83b5-d995ff706507'\n# Azure Machine Learning resource group NOT the managed resource group\nresource_group = 'azureml' \n\n#Azure Machine Learning workspace name, NOT Azure Databricks workspace\nworkspace_name = 'ws01ent'  "],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["### Test with a single store & brand combination (single time series)"],"metadata":{}},{"cell_type":"code","source":["%run ./timeseries_utilities\n"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["#Getting data\nimport pandas as pd\ntrain_data_df = spark.sql(\"select to_timestamp(WeekStarting) WeekStarting, float(Quantity), Brand,Revenue, Store from OJ_Sales_Data where Store = '1066' and Brand ='tropicana'\").toPandas()\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["display(train_data_df.head(10))"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>WeekStarting</th><th>Quantity</th><th>Brand</th><th>Revenue</th><th>Store</th></tr></thead><tbody><tr><td>1990-06-14T00:00:00.000+0000</td><td>13198.0</td><td>tropicana</td><td>29695.5</td><td>1066</td></tr><tr><td>1990-06-21T00:00:00.000+0000</td><td>12188.0</td><td>tropicana</td><td>27179.24</td><td>1066</td></tr><tr><td>1990-06-28T00:00:00.000+0000</td><td>10453.0</td><td>tropicana</td><td>25505.32</td><td>1066</td></tr><tr><td>1990-07-05T00:00:00.000+0000</td><td>13390.0</td><td>tropicana</td><td>35349.6</td><td>1066</td></tr><tr><td>1990-07-12T00:00:00.000+0000</td><td>12798.0</td><td>tropicana</td><td>29691.359999999997</td><td>1066</td></tr><tr><td>1990-07-19T00:00:00.000+0000</td><td>18476.0</td><td>tropicana</td><td>49146.16</td><td>1066</td></tr><tr><td>1990-07-26T00:00:00.000+0000</td><td>16244.0</td><td>tropicana</td><td>35087.04</td><td>1066</td></tr><tr><td>1990-08-02T00:00:00.000+0000</td><td>16057.0</td><td>tropicana</td><td>35807.11</td><td>1066</td></tr><tr><td>1990-08-09T00:00:00.000+0000</td><td>16888.0</td><td>tropicana</td><td>35127.04</td><td>1066</td></tr><tr><td>1990-08-16T00:00:00.000+0000</td><td>14045.0</td><td>tropicana</td><td>30056.300000000003</td><td>1066</td></tr></tbody></table></div>"]}}],"execution_count":29},{"cell_type":"code","source":["\n#Getting data for one table to test the utility function\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport joblib\nimport os\ntarget_column= 'Quantity'\ntimestamp_column= 'WeekStarting'\ntimeseries_id_columns= [ 'Store', 'Brand']\ndrop_columns=['Revenue', 'Store', 'Brand']\nmodel_type= 'lr'\nmodel_name=train_data_df['Store'][0]+\"_\"+train_data_df['Brand'][0]\ntest_size=20\n# 1.0 Read the data from CSV - parse timestamps as datetime type and put the time in the index\ndata = train_data_df \\\n        .set_index('WeekStarting') \\\n        .sort_index(ascending=True)\n\n# 2.0 Split the data into train and test sets\ntrain = data[:-test_size]\ntest = data[-test_size:]\n\n# 3.0 Create and fit the forecasting pipeline\n# The pipeline will drop unhelpful features, make a calendar feature, and make lag features\nlagger = SimpleLagger(target_column, lag_orders=[1, 2, 3, 4])\ntransform_steps = [('column_dropper', ColumnDropper(drop_columns)),\n                   ('calendar_featurizer', SimpleCalendarFeaturizer()), ('lagger', lagger)]\nforecaster = SimpleForecaster(transform_steps, LinearRegression(), target_column, timestamp_column)\nforecaster.fit(train)\nprint('Featurized data example:')\ndisplay(forecaster.transform(train).head())\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Featurized data example:\n/databricks/spark/python/pyspark/sql/pandas/conversion.py:315: UserWarning: createDataFrame attempted Arrow optimization because &#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true; however, failed by the reason below:\n  Unable to convert the field Week_Year. If this column is not necessary, you may consider dropping it or converting to primitive type before the conversion.\nDirect cause: Unsupported type in conversion from Arrow: uint32\nAttempting non-optimization as &#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; is set to true.\n  warnings.warn(msg)\n</div>"]}},{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Quantity</th><th>Week_Year</th><th>lag_1</th><th>lag_2</th><th>lag_3</th><th>lag_4</th></tr></thead><tbody><tr><td>13198.0</td><td>24</td><td>NaN</td><td>NaN</td><td>NaN</td><td>NaN</td></tr><tr><td>12188.0</td><td>25</td><td>13198.0</td><td>NaN</td><td>NaN</td><td>NaN</td></tr><tr><td>10453.0</td><td>26</td><td>12188.0</td><td>13198.0</td><td>NaN</td><td>NaN</td></tr><tr><td>13390.0</td><td>27</td><td>10453.0</td><td>12188.0</td><td>13198.0</td><td>NaN</td></tr><tr><td>12798.0</td><td>28</td><td>13390.0</td><td>10453.0</td><td>12188.0</td><td>13198.0</td></tr></tbody></table></div>"]}}],"execution_count":30},{"cell_type":"code","source":["from azureml.core.authentication import ServicePrincipalAuthentication\nfrom azureml.core import Workspace\nfrom azureml.core import Model\n\nimport cloudpickle \n\nsp_auth = ServicePrincipalAuthentication(tenant_id =tenant_id,\n                                         service_principal_id=service_principal_id,\n                                         service_principal_password=service_principal_password)\n# Instantiate Azure Machine Learning workspace\nws = Workspace.get(name=workspace_name,\n                   subscription_id=subscription_id,\n                   resource_group=resource_group,auth= sp_auth)\n\n\n# 4.0 Get predictions on test set\nforecasts = forecaster.forecast(test)\ncompare_data = test.assign(forecasts=forecasts).dropna()\n\n# 5.0 Calculate accuracy metrics for the fit\nmse = mean_squared_error(compare_data[target_column], compare_data['forecasts'])\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(compare_data[target_column], compare_data['forecasts'])\nactuals = compare_data[target_column].values\npreds = compare_data['forecasts'].values\nmape = np.mean(np.abs((actuals - preds) / actuals) * 100)\n\n# 7.0 Train model with full dataset\nforecaster.fit(data)\n\n# 8.0 Save the forecasting pipeline\nwith open(model_name, mode='wb') as file:\n   cloudpickle.dump(forecaster, file)\n\nmodel = Model.register(workspace=ws, model_name=model_name, model_path=model_name, tags={'mse':str(mse), 'mape': str(mape), 'rmse': str(rmse)})\n\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;command-2941322682361995&gt;:228: DeprecationWarning: The default dtype for empty Series will be &#39;object&#39; instead of &#39;float64&#39; in a future version. Specify a dtype explicitly to silence this warning.\n  forecasts_insamp = pd.Series()\n&lt;command-2941322682361995&gt;:234: DeprecationWarning: The default dtype for empty Series will be &#39;object&#39; instead of &#39;float64&#39; in a future version. Specify a dtype explicitly to silence this warning.\n  forecasts = pd.Series()\nOut[9]: &lt;__main__.SimpleForecaster at 0x7f84badf2f70&gt;</div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["####Scale it up with many model training with function Pandas API"],"metadata":{}},{"cell_type":"code","source":["#Prepare the core training function\n\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nfrom azureml.core import Workspace\nfrom azureml.core import Model\nimport cloudpickle\n#do not use joblib to dump because it will have issue with multi-level object\ndef many_model_train(train_data_df):\n  sp_auth = ServicePrincipalAuthentication(tenant_id =tenant_id,\n                                         service_principal_id=service_principal_id,\n                                         service_principal_password=service_principal_password)\n  # Instantiate Azure Machine Learning workspace\n  ws = Workspace.get(name=workspace_name,\n                     subscription_id=subscription_id,\n                     resource_group=resource_group,auth= sp_auth)\n\n\n  target_column= 'Quantity'\n  timestamp_column= 'WeekStarting'\n  timeseries_id_columns= [ 'Store', 'Brand']\n  drop_columns=['Revenue', 'Store', 'Brand']\n  model_type= 'lr'\n  #Get the store and brand. They are unique from the group so just the first value is sufficient\n  store = train_data_df['Store'][0]\n  brand = train_data_df['Brand'][0]\n\n  model_name=store+\"_\"+brand\n  test_size=20\n  # 1.0 Format the input data from group by, put the time in the index\n  data = train_data_df \\\n          .set_index('WeekStarting') \\\n          .sort_index(ascending=True)\n\n  # 2.0 Split the data into train and test sets\n  train = data[:-test_size]\n  test = data[-test_size:]\n\n  # 3.0 Create and fit the forecasting pipeline\n  # The pipeline will drop unhelpful features, make a calendar feature, and make lag features\n  lagger = SimpleLagger(target_column, lag_orders=[1, 2, 3, 4])\n  transform_steps = [('column_dropper', ColumnDropper(drop_columns)),\n                     ('calendar_featurizer', SimpleCalendarFeaturizer()), ('lagger', lagger)]\n  forecaster = SimpleForecaster(transform_steps, LinearRegression(), target_column, timestamp_column)\n  forecaster.fit(train)\n\n  # 4.0 Get predictions on test set\n  forecasts = forecaster.forecast(test)\n  compare_data = test.assign(forecasts=forecasts).dropna()\n\n  # 5.0 Calculate accuracy metrics for the fit\n  mse = mean_squared_error(compare_data[target_column], compare_data['forecasts'])\n  rmse = np.sqrt(mse)\n  mae = mean_absolute_error(compare_data[target_column], compare_data['forecasts'])\n  actuals = compare_data[target_column].values\n  preds = compare_data['forecasts'].values\n  mape = np.mean(np.abs((actuals - preds) / actuals) * 100)\n\n  # 7.0 Train model with full dataset\n  forecaster.fit(data)\n\n  # 8.0 Save the pipeline and register model to AML\n  with open(model_name, mode='wb') as file:\n     cloudpickle.dump(forecaster, file)#   \n  model = Model.register(workspace=ws, model_name=model_name, model_path=model_name, tags={'mse':str(mse), 'mape': str(mape), 'rmse': str(rmse)})\n  \n  return pd.DataFrame({'Store':store,'Brand':brand, 'mse':[mse], 'mape': [mape], 'rmse': [rmse], 'model_name':[model_name]})\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33},{"cell_type":"code","source":["df = spark.sql(\"select to_timestamp(WeekStarting) WeekStarting, float(Quantity), Brand,Revenue, Store from OJ_Sales_Data\")\ndf = df.repartition(200) #to increase parallelism\nresult = df.groupby([\"Brand\",\"Store\"]).applyInPandas(many_model_train, schema=\"Store string, Brand string, mse float, mape float, rmse float, model_name string \")\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":34},{"cell_type":"code","source":["display(result.head(10))"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Store</th><th>Brand</th><th>mse</th><th>mape</th><th>rmse</th><th>model_name</th></tr></thead><tbody><tr><td>1031</td><td>tropicana</td><td>1.0501595E7</td><td>20.475067138671875</td><td>3240.616455078125</td><td>1031_tropicana</td></tr><tr><td>1021</td><td>minute.maid</td><td>8323296.5</td><td>17.98264503479004</td><td>2885.012451171875</td><td>1021_minute.maid</td></tr><tr><td>1074</td><td>tropicana</td><td>8422692.0</td><td>18.610090255737305</td><td>2902.1875</td><td>1074_tropicana</td></tr><tr><td>1077</td><td>minute.maid</td><td>1.2016312E7</td><td>22.2121524810791</td><td>3466.455322265625</td><td>1077_minute.maid</td></tr><tr><td>1078</td><td>minute.maid</td><td>6714000.0</td><td>13.475154876708984</td><td>2591.138671875</td><td>1078_minute.maid</td></tr><tr><td>1019</td><td>minute.maid</td><td>1.0599569E7</td><td>23.259544372558594</td><td>3255.69775390625</td><td>1019_minute.maid</td></tr><tr><td>1090</td><td>tropicana</td><td>5647451.5</td><td>15.766434669494629</td><td>2376.436767578125</td><td>1090_tropicana</td></tr><tr><td>1099</td><td>tropicana</td><td>8836949.0</td><td>19.134098052978516</td><td>2972.70068359375</td><td>1099_tropicana</td></tr><tr><td>1014</td><td>minute.maid</td><td>7331310.0</td><td>14.901358604431152</td><td>2707.63916015625</td><td>1014_minute.maid</td></tr><tr><td>1020</td><td>minute.maid</td><td>1.0048782E7</td><td>20.763259887695312</td><td>3169.9814453125</td><td>1020_minute.maid</td></tr></tbody></table></div>"]}}],"execution_count":35},{"cell_type":"markdown","source":["###Many Model Inferencing: Can you score using multiple models in parallel?"],"metadata":{}},{"cell_type":"markdown","source":["#### Home work: please prepare a function pandas UDF to produce forecast for mutliple store and brand given the test data"],"metadata":{}},{"cell_type":"markdown","source":["### Solution"],"metadata":{}},{"cell_type":"markdown","source":["#### Quick test the forecast function in utils with just one time series"],"metadata":{}},{"cell_type":"code","source":["#Test forecast for one time series, need to run command 27-30 first\nts_id_dict = {id_col: str(data[id_col].iloc[0]) for id_col in timeseries_id_columns}\nforecasts=forecaster.forecast(data)\nprediction_df = forecasts.to_frame(name='Prediction')\nprediction_df =prediction_df.reset_index().assign(**ts_id_dict)\ndisplay(prediction_df.head(10))"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;command-2941322682361995&gt;:228: DeprecationWarning: The default dtype for empty Series will be &#39;object&#39; instead of &#39;float64&#39; in a future version. Specify a dtype explicitly to silence this warning.\n  forecasts_insamp = pd.Series()\n&lt;command-2941322682361995&gt;:234: DeprecationWarning: The default dtype for empty Series will be &#39;object&#39; instead of &#39;float64&#39; in a future version. Specify a dtype explicitly to silence this warning.\n  forecasts = pd.Series()\n</div>"]}},{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>WeekStarting</th><th>Prediction</th><th>Store</th><th>Brand</th></tr></thead><tbody><tr><td>1990-06-14T00:00:00.000+0000</td><td>null</td><td>1066</td><td>tropicana</td></tr><tr><td>1990-06-21T00:00:00.000+0000</td><td>null</td><td>1066</td><td>tropicana</td></tr><tr><td>1990-06-28T00:00:00.000+0000</td><td>null</td><td>1066</td><td>tropicana</td></tr><tr><td>1990-07-05T00:00:00.000+0000</td><td>null</td><td>1066</td><td>tropicana</td></tr><tr><td>1990-07-12T00:00:00.000+0000</td><td>14410.436058105348</td><td>1066</td><td>tropicana</td></tr><tr><td>1990-07-19T00:00:00.000+0000</td><td>14569.927951313914</td><td>1066</td><td>tropicana</td></tr><tr><td>1990-07-26T00:00:00.000+0000</td><td>14580.11668534655</td><td>1066</td><td>tropicana</td></tr><tr><td>1990-08-02T00:00:00.000+0000</td><td>14873.030648117357</td><td>1066</td><td>tropicana</td></tr><tr><td>1990-08-09T00:00:00.000+0000</td><td>15168.468388925961</td><td>1066</td><td>tropicana</td></tr><tr><td>1990-08-16T00:00:00.000+0000</td><td>14837.636937709074</td><td>1066</td><td>tropicana</td></tr></tbody></table></div>"]}}],"execution_count":40},{"cell_type":"markdown","source":["#### Main solution using map in pandas & loading models from AML workspace"],"metadata":{}},{"cell_type":"code","source":["#Prepare the core forecast function in pandas function API  \n\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nfrom azureml.core import Workspace\nfrom azureml.core import Model\nimport cloudpickle\n#do not use joblib to dump because it will have issue with multi-level object\ndef many_model_forecast(input_data_df):\n  sp_auth = ServicePrincipalAuthentication(tenant_id =tenant_id,\n                                         service_principal_id=service_principal_id,\n                                         service_principal_password=service_principal_password)\n  # Instantiate Azure Machine Learning workspace\n  ws = Workspace.get(name=workspace_name,\n                     subscription_id=subscription_id,\n                     resource_group=resource_group,auth= sp_auth)\n\n\n  target_column= 'Quantity'\n  timestamp_column= 'WeekStarting'\n  timeseries_id_columns= [ 'Store', 'Brand']\n  drop_columns=['Revenue', 'Store', 'Brand']\n  data = input_data_df \\\n        .set_index(timestamp_column) \\\n        .sort_index(ascending=True)\n  #Prepare loading model from Azure ML, get the latest model by default\n  model_name=data['Store'][0]+\"_\"+data['Brand'][0]\n  model = Model(ws, model_name)\n  model.download(exist_ok =True)\n  with open(model_name, 'rb') as f:\n    forecaster = cloudpickle.load(f)\n\n#   Get predictions \n  #This is to append the store and brand column to the result\n  ts_id_dict = {id_col: str(data[id_col].iloc[0]) for id_col in timeseries_id_columns}\n  forecasts=forecaster.forecast(data)\n  prediction_df = forecasts.to_frame(name='Prediction')\n  prediction_df =prediction_df.reset_index().assign(**ts_id_dict)\n  \n  return prediction_df\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":42},{"cell_type":"code","source":["#Load data to score, for now, it's same train data but in reality, it should be different.\ndf = spark.sql(\"select to_timestamp(WeekStarting) WeekStarting, float(Quantity), Brand,Revenue, Store from OJ_Sales_Data\")\ndf = df.repartition(200) #to increase parallelism\nprediction_result = df.groupby([\"Brand\",\"Store\"]).applyInPandas(many_model_forecast, schema=\"WeekStarting date, Store string, Brand string, Prediction float\")"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":43},{"cell_type":"code","source":["display(prediction_result.head(10))"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>WeekStarting</th><th>Store</th><th>Brand</th><th>Prediction</th></tr></thead><tbody><tr><td>1990-06-14</td><td>1031</td><td>tropicana</td><td>null</td></tr><tr><td>1990-06-21</td><td>1031</td><td>tropicana</td><td>null</td></tr><tr><td>1990-06-28</td><td>1031</td><td>tropicana</td><td>null</td></tr><tr><td>1990-07-05</td><td>1031</td><td>tropicana</td><td>null</td></tr><tr><td>1990-07-12</td><td>1031</td><td>tropicana</td><td>13858.3623046875</td></tr><tr><td>1990-07-19</td><td>1031</td><td>tropicana</td><td>13675.5673828125</td></tr><tr><td>1990-07-26</td><td>1031</td><td>tropicana</td><td>14244.9736328125</td></tr><tr><td>1990-08-02</td><td>1031</td><td>tropicana</td><td>14784.28515625</td></tr><tr><td>1990-08-09</td><td>1031</td><td>tropicana</td><td>14830.6640625</td></tr><tr><td>1990-08-16</td><td>1031</td><td>tropicana</td><td>14164.19140625</td></tr></tbody></table></div>"]}}],"execution_count":44},{"cell_type":"markdown","source":["##### Small ask: can you add a actual qty column to the result if the data to score has it?"],"metadata":{}}],"metadata":{"name":"many_models_spark","notebookId":3319412928880917},"nbformat":4,"nbformat_minor":0}